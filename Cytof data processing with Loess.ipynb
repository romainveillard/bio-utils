{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cytof data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "For compatibility, the `numpy` version needs to be fixed:\n",
    "```bash\n",
    "pip3 install pandas numpy scprep statsmodels seaborn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the folder containing the CSV files and the output file name\n",
    "folder_path = '/Users/romain/Documents/Perso/aure/samples'\n",
    "\n",
    "#This is a single file containing all conditions. Files have been downloaded from Cytobank as txt files, then concatenated together using R.\n",
    "normalisation_channel = '165Ho_AlexaFluor488'\n",
    "\n",
    "# Specify the metadata columns\n",
    "# metadata_columns = ['Cell_Index', 'Condition', 'Control', 'Replicate', normalisation_channel]\n",
    "metadata_columns = ['Cell_Index', normalisation_channel]\n",
    "\n",
    "# Specify other columns to exclude from processing\n",
    "excluded_columns = []\n",
    "\n",
    "# Compute the non data columns in a new variable for easier later use\n",
    "non_data_columns = excluded_columns + metadata_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import common packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', 60)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate all files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_condition_replicate_from_filename = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events = pd.DataFrame()\n",
    "\n",
    "# Loop over all files in folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Only consider files with '.txt' extension\n",
    "    if filename.endswith('.txt'):\n",
    "        # Build the full path to file\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        # Load the file\n",
    "        events = pd.read_csv(file_path, delimiter='\\t')\n",
    "\n",
    "        if get_condition_replicate_from_filename:\n",
    "            # Retrieve metadata from the filename (ex: WGANormalised_Pro_PDO21 + CAFs_01.fcs_file_internal...)\n",
    "            # First split: ['Pro_AD022_AD_PDO21_OldMG_DMSO_02', '_file_internal']\n",
    "            # Second split over first element: ['Pro', 'AD022', 'AD', 'PDO21+CAFs', 'OldMG', 'DMSO', '02']\n",
    "\n",
    "            # Pro_AD022_AD_PDO21+CAFs_OldMG_DMSO_02.fcs_file_internal_comp_PDOs\n",
    "            metadata_from_filename = filename.split('.fcs')[0].split('_')\n",
    "            replicate = metadata_from_filename[-1] # 02\n",
    "            drug = metadata_from_filename[-2] # DMSO\n",
    "            treatment = metadata_from_filename[-3] # OldMG\n",
    "            culture = metadata_from_filename[-4] # PDO21+CAFs\n",
    "            events['Culture'] = culture\n",
    "            events['Treatment'] = treatment\n",
    "            # Store the control name in the dataframe: second-to-last element split over '+', and stripped to remove whitespace from both sides\n",
    "            # events['Control'] = culture.split('+')[0].strip()\n",
    "            # Store the replicate in the dataframe: last element\n",
    "            events['Replicate'] = replicate # 02\n",
    "            events['Drug'] = drug # suposed to be DMSO\n",
    "            # events['Condition'] = culture + '_' + treatment + '_' + replicate\n",
    "            metadata_columns += ['Culture', 'Treatment', 'Replicate', 'Drug']\n",
    "\n",
    "\n",
    "        # Add the file data to the DataFrame containing all events\n",
    "        all_events = pd.concat([all_events, events], ignore_index=True)\n",
    "\n",
    "metadata_columns = list(set(metadata_columns))\n",
    "# Print all events\n",
    "initial_all_events = all_events.copy()\n",
    "all_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Advantages of Loess-Based Correction\n",
    "#\n",
    "# ✅ Handles Non-Linearity: No assumption about the functional form of the relationship.\n",
    "# ✅ Data-Driven Approach: Adapts to trends within the dataset.\n",
    "# ✅ Preserves Biological Signal: Corrects only for size effects without distorting underlying biological differences.\n",
    "#\n",
    "# Challenges & Considerations\n",
    "#\n",
    "# ⚠ Computational Cost: Loess smoothing can be slow for large datasets.\n",
    "# ⚠ Choosing the Smoothing Parameter (span or frac): Needs tuning to avoid overfitting or underfitting.\n",
    "# ⚠ Boundary Effects: Can be unstable at extreme values of WGA.\n",
    "def apply_loess_correction(maker_data, wga_data):\n",
    "    # Fit Loess model\n",
    "    lowess_fit = sm.nonparametric.lowess(maker_data, wga_data, frac=0.5)\n",
    "\n",
    "    # Interpolating fitted values\n",
    "    # - Column 0 (lowess_fit[:, 0]): The independent variable values (e.g., WGA, the size marker). | WGA values (sorted).\n",
    "\t# - Column 1 (lowess_fit[:, 1]): The corresponding smoothed (Loess-fitted) dependent variable values (e.g., marker intensity). | Loess-smoothed marker intensity values.\n",
    "    fitted_values = np.interp(wga_data, lowess_fit[:, 0], lowess_fit[:, 1])\n",
    "\n",
    "    # Return the corrected data\n",
    "    return maker_data / fitted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Separate the columns to normalize and the columns to leave as-is\n",
    "columns_to_normalize = all_events.columns.difference(metadata_columns).tolist()\n",
    "\n",
    "norm_channel_data = all_events[normalisation_channel]\n",
    "\n",
    "# Apply Loess correction to each marker column\n",
    "# Use joblib to parallelize\n",
    "results = Parallel(n_jobs=-1, backend='loky')(  \n",
    "    # n_jobs=-1: Use all available cores\n",
    "    # backend='loky': Loky backend for robust parallelization\n",
    "    delayed(apply_loess_correction)(all_events[marker], norm_channel_data)\n",
    "    for marker in columns_to_normalize\n",
    ")\n",
    "\n",
    "# Filter to keep only rows with all finite values\n",
    "# all_events = all_events[np.isfinite(all_events).all(axis=1)]\n",
    "\n",
    "corrected_events = pd.concat(results, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMD Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events = initial_all_events.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the metadata to create a df with only numerical data for normalisation/transformation\n",
    "data = all_events.drop(metadata_columns ,axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure all metadata columns are strings (not numberical as this will run into errors)\n",
    "metadata = all_events.filter(metadata_columns)\n",
    "metadata[metadata_columns] = metadata[metadata_columns].applymap(str)\n",
    "metadata  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a subset of data (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batches:\n",
    "#Batch 1 = PDO27wt/ko exp B BM/MOPC21/B7C18\n",
    "#Batch 2 = PDO27 ABCEDF7 Tr\n",
    "#Batch 3 = PDO27 ABCDEF7 NT\n",
    "#Batch 4 = PDO21/23/216 ABE7 Tr\n",
    "#Batch 5 = PDO21/23/216 ABE7 NT \n",
    "#Batch 6 = PDO5/11 ABE7 Tr/NT\n",
    "#Batch 7 = PDO75/99 ABE7 Tr/NT\n",
    "#Batch 8 = PDO109/141 ABE7 Tr/NT\n",
    "#Batch 9 = NT/eGFP/eGFP-stIL15 ABE7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To enable this process, set this variable to True, False otherwise\n",
    "should_select_a_subset = False\n",
    "\n",
    "subset_condition = [True]\n",
    "if should_select_a_subset:\n",
    "    # Define here the filter to apply\n",
    "    subset_condition = \\\n",
    "        metadata['Patient'].isin(['X','5','11','21','23','27','75','99','109','141','216']) & \\\n",
    "        metadata['gd_donor'].isin(['A','B','E','7']) & \\\n",
    "        metadata['Transduction'].isin(['eGFP-stIL15']) & \\\n",
    "        metadata['Treatment'].isin(['BM','B7C18']) & \\\n",
    "        metadata['Batch'].isin(['Batch2','Batch4','Batch6','Batch7','Batch8'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if should_select_a_subset:\n",
    "    #Select eGFP-stIL15 / ABE7 / wt PDO / BM / B7C18 (I was just selecting the data I wanted to use)\n",
    "    data = data.loc[subset_condition]\n",
    "    data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if should_select_a_subset:\n",
    "    #selecting the corresponding metadata\n",
    "    metadata = metadata.loc[subset_condition]\n",
    "    metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arcsinh transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcsinh_cofactor = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arcsinh transformation of all raw data\n",
    "data = np.arcsinh(data/arcsinh_cofactor)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch effect correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scprep.normalize\n",
    "\n",
    "# Data centering by batch to correct any cytof batch effect\n",
    "# Only if 'Batch' is a metadata\n",
    "if 'Batch' in metadata.columns:\n",
    "    data = scprep.normalize.batch_mean_center(data,sample_idx=metadata['Batch'])\n",
    "    data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-assemble processed data with metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate data with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine arcsinh-transformed and mean-centered data with metadata again\n",
    "processed_data = pd.concat([data, metadata], axis=1)\n",
    "processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-index the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_count = processed_data.shape[0]\n",
    "processed_data.index = np.arange(row_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure type of metadata column to be string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processed_data[metadata_columns] = processed_data[metadata_columns].applymap(str)\n",
    "processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the `Condition` information (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_colmns = ['Culture', 'Treatment', 'Drug', 'Replicate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the `Condition` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Condition' not in metadata.columns:\n",
    "    # Create a condition column for every cell in the experiment\n",
    "    processed_data['Condition'] = processed_data[condition_colmns].astype(str).agg('_'.join, axis=1)\n",
    "\n",
    "    # Add `Condition` to the list of metadata columns\n",
    "    metadata_columns += ['Condition']\n",
    "\n",
    "processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the `Control` information (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_culture = 'PDO21'\n",
    "control_columns = ['Treatment']\n",
    "control_drug = 'DMSO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the `Control` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Control' not in metadata.columns:\n",
    "    # Define control for pairwise EMD. \n",
    "    # processed_data['Control'] = processed_data[control_columns].astype(str).agg('_'.join, axis=1)\n",
    "    processed_data['Control'] = control_culture + \"_\" + processed_data[control_columns].astype(str).agg('_'.join, axis=1) + \"_\" + control_drug\n",
    "\n",
    "    # Add `Control` to the list of metadata columns\n",
    "    metadata_columns += ['Control']\n",
    "\n",
    "processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save processed data to file (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.to_csv(folder_path + '/processed_data.bkp', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise EMD dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the markers list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each column in the Dataframe, keep only the ones not in the `metadata_columns` variable\n",
    "markers_list = [col for col in processed_data.columns if col not in metadata_columns]\n",
    "# marker_list = list(processed_data.columns.values)\n",
    "markers_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the conditions list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of unique conditions\n",
    "conditions_list = pd.unique(processed_data['Condition'].tolist())\n",
    "conditions_list\n",
    "\n",
    "# pdo21 et pdo21+caf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the controls list (unused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of unique controls\n",
    "controls_list = pd.unique(processed_data['Control'].tolist())\n",
    "controls_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the DataFrame that will receive the EMD values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty df with NaN values to populate with the EMD values\n",
    "emd_dataframe = pd.DataFrame(\n",
    "    np.full(\n",
    "        (len(conditions_list), len(markers_list)), \n",
    "        np.nan),\n",
    "    columns = markers_list,\n",
    "    index = conditions_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate EMD scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scprep.stats\n",
    "\n",
    "# Precompute control dataframes for each control name\n",
    "control_data = {\n",
    "    control: processed_data.loc[processed_data[\"Condition\"].str.startswith(control)].copy()\n",
    "    for control in processed_data['Control'].unique()\n",
    "}\n",
    "\n",
    "# Function to process a single (condition, marker) pair\n",
    "def process_condition_marker(condition, marker):\n",
    "    # Dataframe of all events for the condition in the list\n",
    "    condition_events = processed_data.loc[processed_data[\"Condition\"] == condition].copy()\n",
    "    control_name = condition_events['Control'].values[0]\n",
    "    # Dataframe of all events from the control that will be compared with the events of the current condition\n",
    "    control_df = control_data[control_name].copy()\n",
    "    \n",
    "    condition_median = condition_events[marker].median()\n",
    "    control_median = control_df[marker].median()\n",
    "\n",
    "    # Check the sign by using the `median` values\n",
    "    sign = np.sign(condition_median - control_median)\n",
    "    \n",
    "    # Fall back to mean if medians are equal\n",
    "    if sign == 0:\n",
    "        sign = np.sign(\n",
    "            condition_events[marker].mean() - control_df[marker].mean()\n",
    "        )\n",
    "    \n",
    "    # Compute the EMD\n",
    "    emd = scprep.stats.EMD(condition_events[marker], control_df[marker])\n",
    "    return (condition, marker, sign * emd)\n",
    "\n",
    "\n",
    "# Use joblib to parallelize\n",
    "results = Parallel(n_jobs=-1, backend='loky')(  \n",
    "    # n_jobs=-1: Use all available cores\n",
    "    # backend='loky': Loky backend for robust parallelization\n",
    "    delayed(process_condition_marker)(condition, marker)\n",
    "    for condition in conditions_list\n",
    "    for marker in markers_list\n",
    ")\n",
    "\n",
    "# Convert results into a DataFrame\n",
    "emd_dataframe = (\n",
    "    pd.DataFrame(results, columns=['Condition', 'Marker', 'Signed EMD'])\n",
    "    .pivot(index='Condition', columns='Marker', values='Signed EMD')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emd_dataframe.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save EMD to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emd_dataframe.to_csv(folder_path + '/events_emd.bkp', index=True, sep='\\t')\n",
    "emd_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# import phate\n",
    "# import matplotlib\n",
    "# import matplotlib.colors as mcolors\n",
    "# import scprep\n",
    "# import os\n",
    "# import scipy\n",
    "import seaborn as sns\n",
    "# from scipy.stats.stats import pearsonr\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# generate custom colour palette for seaborn heatmaps\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Define custom colormap with white at zero\n",
    "custom_diverging = mcolors.LinearSegmentedColormap.from_list(\"\",[\"#6f00ff\", \"white\", \"#ef3038\"], N=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 12))\n",
    "\n",
    "offset = mcolors.TwoSlopeNorm(vmin=-1 ,vcenter=0, vmax=1)\n",
    "\n",
    "sns.heatmap(emd_dataframe.T, cmap=custom_diverging, norm=offset, )\n",
    "\n",
    "plt.savefig(folder_path + '/emd_all_global.png', dpi=900, transparent=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
